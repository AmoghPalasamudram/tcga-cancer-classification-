{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing for ML\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll prepare the TCGA data for machine learning:\n",
    "\n",
    "1. **Handle high dimensionality** - Select the most informative genes\n",
    "2. **Normalize the data** - Scale features for ML algorithms\n",
    "3. **Split data** - Create train/test sets with proper stratification\n",
    "\n",
    "### Why Preprocessing Matters\n",
    "- 20,000 features vs 801 samples = risk of overfitting\n",
    "- Many genes don't vary much (uninformative)\n",
    "- Different genes have different scales\n",
    "- Need proper train/test split to evaluate honestly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data_loader import load_tcga_data, CANCER_TYPE_INFO\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y, gene_names, sample_ids = load_tcga_data(verbose=True)\n",
    "print(f\"\\nOriginal shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split\n",
    "\n",
    "**Important**: Split BEFORE any preprocessing to avoid data leakage!\n",
    "\n",
    "We use stratified split to maintain class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Label encoding:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label} -> {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_encoded  # Maintain class proportions\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nClass distribution:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    train_pct = (y_train == i).sum() / len(y_train) * 100\n",
    "    test_pct = (y_test == i).sum() / len(y_test) * 100\n",
    "    print(f\"  {label}: Train {train_pct:.1f}%, Test {test_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection Strategy\n",
    "\n",
    "We'll use a multi-step approach:\n",
    "1. **Variance filtering** - Remove near-constant genes\n",
    "2. **Statistical selection** - Keep genes most associated with cancer type\n",
    "\n",
    "### Why not use PCA for classification?\n",
    "PCA is great for visualization, but for classification we want:\n",
    "- Interpretable features (real genes, not abstract components)\n",
    "- Supervised selection (genes that predict the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove low-variance genes\n",
    "# Only use training data to determine threshold!\n",
    "\n",
    "gene_variance = np.var(X_train, axis=0)\n",
    "\n",
    "# Use median variance as threshold (removes ~50% of least variable genes)\n",
    "variance_threshold = np.percentile(gene_variance, 50)\n",
    "print(f\"Variance threshold (50th percentile): {variance_threshold:.4f}\")\n",
    "\n",
    "# Apply variance filter\n",
    "var_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_train_var = var_selector.fit_transform(X_train)\n",
    "X_test_var = var_selector.transform(X_test)\n",
    "\n",
    "# Track which genes remain\n",
    "var_mask = var_selector.get_support()\n",
    "genes_after_var = [g for g, m in zip(gene_names, var_mask) if m]\n",
    "\n",
    "print(f\"\\nGenes before: {X_train.shape[1]}\")\n",
    "print(f\"Genes after variance filter: {X_train_var.shape[1]}\")\n",
    "print(f\"Removed: {X_train.shape[1] - X_train_var.shape[1]} low-variance genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Select top K genes using ANOVA F-test\n",
    "# F-test measures how well each gene separates the cancer types\n",
    "\n",
    "K_FEATURES = 1000  # Keep top 1000 genes (good balance)\n",
    "\n",
    "# Fit selector on training data only\n",
    "kbest_selector = SelectKBest(score_func=f_classif, k=K_FEATURES)\n",
    "X_train_selected = kbest_selector.fit_transform(X_train_var, y_train)\n",
    "X_test_selected = kbest_selector.transform(X_test_var)\n",
    "\n",
    "# Get selected gene names\n",
    "kbest_mask = kbest_selector.get_support()\n",
    "selected_genes = [g for g, m in zip(genes_after_var, kbest_mask) if m]\n",
    "\n",
    "print(f\"Final feature count: {X_train_selected.shape[1]}\")\n",
    "print(f\"\\nTop 20 genes by F-score:\")\n",
    "\n",
    "# Get F-scores for selected genes\n",
    "f_scores = kbest_selector.scores_[kbest_mask]\n",
    "gene_scores = list(zip(selected_genes, f_scores))\n",
    "gene_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for gene, score in gene_scores[:20]:\n",
    "    print(f\"  {gene}: {score:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize F-score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# All F-scores\n",
    "ax1 = axes[0]\n",
    "ax1.hist(kbest_selector.scores_, bins=50, alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(np.sort(kbest_selector.scores_)[-K_FEATURES], color='red', \n",
    "            linestyle='--', label=f'Selection threshold')\n",
    "ax1.set_xlabel('F-score', fontsize=12)\n",
    "ax1.set_ylabel('Number of Genes', fontsize=12)\n",
    "ax1.set_title('Distribution of F-scores (ANOVA)', fontsize=14)\n",
    "ax1.legend()\n",
    "\n",
    "# Top genes\n",
    "ax2 = axes[1]\n",
    "top_20 = gene_scores[:20]\n",
    "ax2.barh([g[0] for g in top_20][::-1], [g[1] for g in top_20][::-1], color='steelblue')\n",
    "ax2.set_xlabel('F-score', fontsize=12)\n",
    "ax2.set_title('Top 20 Most Discriminative Genes', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/feature_selection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling\n",
    "\n",
    "Standardization (z-score normalization) centers data to mean=0, std=1.\n",
    "\n",
    "Important for algorithms like SVM, Logistic Regression, and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_final = scaler.fit_transform(X_train_selected)\n",
    "X_test_final = scaler.transform(X_test_selected)\n",
    "\n",
    "print(\"After scaling:\")\n",
    "print(f\"  Training mean: {X_train_final.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Training std: {X_train_final.std():.6f} (should be ~1)\")\n",
    "print(f\"  Test mean: {X_test_final.mean():.6f}\")\n",
    "print(f\"  Test std: {X_test_final.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Preprocessing with PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick PCA to verify data still has good class separation\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_final)\n",
    "X_test_pca = pca.transform(X_test_final)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = sns.color_palette('husl', len(label_encoder.classes_))\n",
    "\n",
    "# Training data\n",
    "ax1 = axes[0]\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    mask = y_train == i\n",
    "    ax1.scatter(X_train_pca[mask, 0], X_train_pca[mask, 1], \n",
    "                c=[colors[i]], label=label, alpha=0.6, s=50)\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax1.set_title('Training Data (Preprocessed)', fontsize=14)\n",
    "ax1.legend()\n",
    "\n",
    "# Test data\n",
    "ax2 = axes[1]\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    mask = y_test == i\n",
    "    ax2.scatter(X_test_pca[mask, 0], X_test_pca[mask, 1], \n",
    "                c=[colors[i]], label=label, alpha=0.6, s=50)\n",
    "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax2.set_title('Test Data (Preprocessed)', fontsize=14)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/preprocessed_pca.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Good class separation preserved after feature selection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything needed for model training\n",
    "import pickle\n",
    "\n",
    "# Data\n",
    "np.save('../data/processed/X_train.npy', X_train_final)\n",
    "np.save('../data/processed/X_test.npy', X_test_final)\n",
    "np.save('../data/processed/y_train.npy', y_train)\n",
    "np.save('../data/processed/y_test.npy', y_test)\n",
    "\n",
    "# Gene names (for interpretation)\n",
    "with open('../data/processed/selected_genes.pkl', 'wb') as f:\n",
    "    pickle.dump(selected_genes, f)\n",
    "\n",
    "# Label encoder (for converting predictions back to names)\n",
    "with open('../data/processed/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Preprocessing objects (for new data)\n",
    "with open('../data/processed/preprocessors.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'var_selector': var_selector,\n",
    "        'kbest_selector': kbest_selector,\n",
    "        'scaler': scaler\n",
    "    }, f)\n",
    "\n",
    "print(\"Saved preprocessed data and pipeline objects!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - X_train.npy, X_test.npy (feature matrices)\")\n",
    "print(\"  - y_train.npy, y_test.npy (labels)\")\n",
    "print(\"  - selected_genes.pkl (gene names for interpretation)\")\n",
    "print(\"  - label_encoder.pkl (convert numeric labels to cancer names)\")\n",
    "print(\"  - preprocessors.pkl (for preprocessing new samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"INPUT:\")\n",
    "print(f\"  - {X.shape[0]} samples x {X.shape[1]:,} genes\")\n",
    "print()\n",
    "print(\"STEPS:\")\n",
    "print(f\"  1. Train/Test Split: 80/20 stratified\")\n",
    "print(f\"  2. Variance Filter: Removed {X_train.shape[1] - X_train_var.shape[1]:,} low-variance genes\")\n",
    "print(f\"  3. SelectKBest: Kept top {K_FEATURES} genes by F-score\")\n",
    "print(f\"  4. StandardScaler: Normalized to mean=0, std=1\")\n",
    "print()\n",
    "print(\"OUTPUT:\")\n",
    "print(f\"  - Training: {X_train_final.shape[0]} samples x {X_train_final.shape[1]} features\")\n",
    "print(f\"  - Test: {X_test_final.shape[0]} samples x {X_test_final.shape[1]} features\")\n",
    "print()\n",
    "print(\"READY FOR MODEL TRAINING!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
