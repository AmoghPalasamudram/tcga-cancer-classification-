{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Model Training and Evaluation\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll train and compare multiple ML models for cancer classification:\n",
    "\n",
    "1. **Baseline**: Logistic Regression\n",
    "2. **Tree-based**: Random Forest, XGBoost\n",
    "3. **SVM**: Support Vector Machine\n",
    "\n",
    "We'll also:\n",
    "- Use cross-validation for robust evaluation\n",
    "- Analyze feature importance (which genes matter most)\n",
    "- Create publication-quality visualizations\n",
    "- Interpret results biologically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, auc, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# For SHAP (model interpretation)\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "X_train = np.load('../data/processed/X_train.npy')\n",
    "X_test = np.load('../data/processed/X_test.npy')\n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "y_test = np.load('../data/processed/y_test.npy')\n",
    "\n",
    "with open('../data/processed/selected_genes.pkl', 'rb') as f:\n",
    "    selected_genes = pickle.load(f)\n",
    "\n",
    "with open('../data/processed/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "n_classes = len(class_names)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Test data: {X_test.shape}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Models\n",
    "\n",
    "We'll compare several commonly used classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with reasonable hyperparameters\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        random_state=RANDOM_STATE,\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        C=1.0\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'SVM (RBF)': SVC(\n",
    "        kernel='rbf',\n",
    "        C=10,\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Models defined:\")\n",
    "for name in models:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation Comparison\n",
    "\n",
    "First, let's compare all models using 5-fold cross-validation on training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Store results\n",
    "cv_results = {}\n",
    "\n",
    "print(\"Running 5-fold cross-validation...\\n\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    cv_results[name] = scores\n",
    "    print(f\"Accuracy: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "cv_df_melted = cv_df.melt(var_name='Model', value_name='Accuracy')\n",
    "\n",
    "sns.boxplot(data=cv_df_melted, x='Model', y='Accuracy', palette='husl', ax=ax)\n",
    "sns.stripplot(data=cv_df_melted, x='Model', y='Accuracy', color='black', alpha=0.5, ax=ax)\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_xlabel('')\n",
    "ax.set_title('5-Fold Cross-Validation Results', fontsize=14)\n",
    "ax.set_ylim([0.9, 1.01])\n",
    "\n",
    "# Add mean values as text\n",
    "for i, name in enumerate(models.keys()):\n",
    "    mean_acc = cv_results[name].mean()\n",
    "    ax.text(i, mean_acc + 0.008, f'{mean_acc:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/cv_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Final Models and Evaluate on Test Set\n",
    "\n",
    "Now let's train on all training data and evaluate on the held-out test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models on full training set\n",
    "trained_models = {}\n",
    "test_results = {}\n",
    "\n",
    "print(\"Training on full training set and evaluating on test set...\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    \n",
    "    # Fit on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Multi-class ROC-AUC\n",
    "    y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "    roc_auc = roc_auc_score(y_test_bin, y_prob, average='weighted', multi_class='ovr')\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    }\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(test_results.keys()),\n",
    "    'CV Accuracy (mean)': [cv_results[m].mean() for m in test_results.keys()],\n",
    "    'CV Accuracy (std)': [cv_results[m].std() for m in test_results.keys()],\n",
    "    'Test Accuracy': [test_results[m]['accuracy'] for m in test_results.keys()],\n",
    "    'Test ROC-AUC': [test_results[m]['roc_auc'] for m in test_results.keys()]\n",
    "})\n",
    "\n",
    "results_df = results_df.round(4)\n",
    "print(\"\\nMODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Analysis of Best Model\n",
    "\n",
    "Let's analyze the best performing model in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on test accuracy\n",
    "best_model_name = max(test_results, key=lambda x: test_results[x]['accuracy'])\n",
    "best_model = trained_models[best_model_name]\n",
    "best_results = test_results[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {best_results['accuracy']:.4f}\")\n",
    "print(f\"Test ROC-AUC: {best_results['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nCLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, best_results['y_pred'], target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_results['y_pred'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute numbers\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax1.set_title(f'Confusion Matrix ({best_model_name})', fontsize=14)\n",
    "\n",
    "# Normalized (percentages)\n",
    "ax2 = axes[1]\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.1f', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
    "ax2.set_ylabel('True Label', fontsize=12)\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax2.set_title('Confusion Matrix (Normalized %)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ROC Curves (Multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve for each class\n",
    "y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "y_prob = best_results['y_prob']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = sns.color_palette('husl', n_classes)\n",
    "\n",
    "for i, (color, class_name) in enumerate(zip(colors, class_names)):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "    roc_auc_i = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=color, lw=2, \n",
    "            label=f'{class_name} (AUC = {roc_auc_i:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title(f'ROC Curves - {best_model_name}', fontsize=14)\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Which genes are most important for classification? This is key for biological interpretation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest (has built-in importance)\n",
    "rf_model = trained_models['Random Forest']\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'gene': selected_genes,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"TOP 30 MOST IMPORTANT GENES (Random Forest)\")\n",
    "print(\"=\" * 50)\n",
    "print(importance_df.head(30).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "top_n = 25\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.barh(range(top_n), top_features['importance'].values[::-1], color='steelblue')\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(top_features['gene'].values[::-1])\n",
    "ax.set_xlabel('Feature Importance', fontsize=12)\n",
    "ax.set_title(f'Top {top_n} Most Important Genes for Cancer Classification', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expression patterns of top genes across cancer types\n",
    "top_genes = importance_df.head(10)['gene'].values\n",
    "top_gene_idx = [selected_genes.index(g) for g in top_genes]\n",
    "\n",
    "# Get expression data for top genes\n",
    "X_top = X_train[:, top_gene_idx]\n",
    "\n",
    "# Calculate mean expression per cancer type\n",
    "mean_expr = pd.DataFrame(X_top, columns=top_genes)\n",
    "mean_expr['cancer'] = [class_names[i] for i in y_train]\n",
    "mean_expr_grouped = mean_expr.groupby('cancer').mean()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(mean_expr_grouped.T, cmap='RdBu_r', center=0, annot=True, fmt='.2f')\n",
    "plt.title('Mean Expression of Top Predictive Genes by Cancer Type', fontsize=14)\n",
    "plt.xlabel('Cancer Type', fontsize=12)\n",
    "plt.ylabel('Gene', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/top_genes_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SHAP Analysis (Model Interpretability)\n",
    "\n",
    "SHAP values explain why the model made each prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    print(\"Computing SHAP values (this may take a minute)...\")\n",
    "    \n",
    "    # Use a subset of test data for faster computation\n",
    "    X_explain = X_test[:50]\n",
    "    \n",
    "    # Create explainer for Random Forest\n",
    "    explainer = shap.TreeExplainer(rf_model)\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    \n",
    "    print(\"SHAP values computed!\")\n",
    "else:\n",
    "    print(\"Skipping SHAP analysis (shap not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Summary plot for each class\n",
    "    fig, axes = plt.subplots(1, n_classes, figsize=(20, 6))\n",
    "    \n",
    "    for i, (ax, class_name) in enumerate(zip(axes, class_names)):\n",
    "        plt.sca(ax)\n",
    "        shap.summary_plot(shap_values[i], X_explain, \n",
    "                         feature_names=selected_genes, \n",
    "                         max_display=10, show=False)\n",
    "        ax.set_title(f'{class_name}', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/shap_summary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model\n",
    "model_path = f'../models/{best_model_name.lower().replace(\" \", \"_\")}_best.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "# Save all trained models\n",
    "for name, model in trained_models.items():\n",
    "    path = f'../models/{name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")}.joblib'\n",
    "    joblib.dump(model, path)\n",
    "    \n",
    "print(f\"\\nAll models saved to models/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PROJECT SUMMARY: TCGA Pan-Cancer Classification\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"DATASET:\")\n",
    "print(f\"  - 801 tumor samples from 5 cancer types\")\n",
    "print(f\"  - Original features: 20,531 genes\")\n",
    "print(f\"  - Selected features: 1,000 genes (variance + F-test selection)\")\n",
    "print()\n",
    "print(\"BEST MODEL:\", best_model_name)\n",
    "print(f\"  - Test Accuracy: {best_results['accuracy']:.2%}\")\n",
    "print(f\"  - Test ROC-AUC: {best_results['roc_auc']:.4f}\")\n",
    "print()\n",
    "print(\"PER-CLASS PERFORMANCE:\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, best_results['y_pred'])\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"  {class_name}: Precision={precision[i]:.2f}, Recall={recall[i]:.2f}, F1={f1[i]:.2f}\")\n",
    "print()\n",
    "print(\"TOP 5 PREDICTIVE GENES:\")\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {i+1}. {row['gene']} (importance: {row['importance']:.4f})\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"  - Gene expression patterns can accurately classify cancer types\")\n",
    "print(\"  - Ensemble methods (RF, XGBoost) perform well on this data\")\n",
    "print(\"  - Top predictive genes are biologically meaningful markers\")\n",
    "print(\"  - This demonstrates the power of ML in cancer genomics\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for the README\n",
    "results_df.to_csv('../results/model_comparison.csv', index=False)\n",
    "importance_df.to_csv('../results/feature_importance.csv', index=False)\n",
    "\n",
    "print(\"Results saved to results/ directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
